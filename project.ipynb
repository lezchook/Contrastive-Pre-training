{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "DATASET_NAME = \"sberquad\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "#model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "passages = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    query = train_data[i][\"question\"]\n",
    "    queries.append(query)\n",
    "    \n",
    "    passage = train_data[i][\"context\"]\n",
    "    passages.append(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(queries, passages, batch_size):\n",
    "    batches = []\n",
    "    pending = []\n",
    "    current_batch = {\"question\": [], \"context\": []}\n",
    "    current_passages = set()\n",
    "\n",
    "    def process_pending():\n",
    "        nonlocal current_batch, current_passages, pending\n",
    "        new_pending = []\n",
    "        for q, p in pending:\n",
    "            if p not in current_passages and len(current_batch[\"question\"]) < batch_size:\n",
    "                current_batch[\"question\"].append(q)\n",
    "                current_batch[\"context\"].append(p)\n",
    "                current_passages.add(p)\n",
    "            else:\n",
    "                new_pending.append((q, p))\n",
    "        pending = new_pending\n",
    "\n",
    "    for q, p in zip(queries, passages):\n",
    "        if p in current_passages:\n",
    "            pending.append((q, p))\n",
    "        else:\n",
    "            current_batch[\"question\"].append(q)\n",
    "            current_batch[\"context\"].append(p)\n",
    "            current_passages.add(p)\n",
    "\n",
    "        if len(current_batch[\"question\"]) == batch_size:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = {\"question\": [], \"context\": []}\n",
    "            current_passages = set()\n",
    "            process_pending()\n",
    "    \n",
    "    \n",
    "    while pending and len(current_batch[\"question\"]) < batch_size:\n",
    "        q, p = pending.pop(0)\n",
    "        if p not in current_passages:\n",
    "            current_batch[\"question\"].append(q)\n",
    "            current_batch[\"context\"].append(p)\n",
    "            current_passages.add(p)\n",
    "        else:\n",
    "            pending.append((q, p))\n",
    "            if all(p in current_passages for _, p in pending):\n",
    "                break\n",
    "            \n",
    "    if current_batch[\"question\"]:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "\n",
    "    def forward(self, query, passage, negative_passages, temperature):\n",
    "        s_positive = F.cosine_similarity(query, passage, dim=-1) / temperature\n",
    "        s_negative = F.cosine_similarity(query.unsqueeze(1), negative_passages, dim=-1) / temperature\n",
    "\n",
    "        exp_for_sum = torch.cat([s_positive.unsqueeze(-1), s_negative], dim=-1)\n",
    "        log_exp_sum = torch.logsumexp(exp_for_sum, dim=-1)\n",
    "        \n",
    "        return (-s_positive + log_exp_sum).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "train_data_batched = get_batches(queries, passages, BATCH_SIZE)\n",
    "trainloader = DataLoader(train_data_batched, batch_size=None, collate_fn=lambda x: x, shuffle=True)\n",
    "loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(trainloader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1652510b718146f3b71b78c5a4da56fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.21011831918678714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6537e55cc07042fbba1ee35a960b811e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0638824780761017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6f9a2c7d9641ceb312e408289a7540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.01893964079820346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4039db2b17a411b80d12b083301b4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.010464606132438225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bfd274282b425987ba80f1e99dcd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0057383512934275184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeeeee0d4954ddfb5e9fa5078966dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.002681931875744408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1e8dd496c84820890cadd0f9572f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.002074575794624848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed836b9e1f4045589f41f9c71e17d3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.00044417231994478855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51592256f9334eb9908ab8eb3bee7eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0002492921500556454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a97dcb32b694009b64eff2fcc9b2417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 5.8491878814859676e-05\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    progressBar = tqdm(range(len(trainloader)), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in trainloader:\n",
    "        query = tokenizer(batch[\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        passage = tokenizer(batch[\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "\n",
    "        query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "        passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        negative_passages = []\n",
    "        for i in range(len(passage_emb)):\n",
    "            negatives = torch.cat([passage_emb[:i], passage_emb[i + 1:]])\n",
    "            negative_passages.append(negatives)\n",
    "\n",
    "        negative_passages = torch.stack(negative_passages)\n",
    "\n",
    "        loss = loss_function(query_emb, passage_emb, negative_passages, 0.01)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progressBar.update(1)\n",
    "        total_loss += loss.item()\n",
    "        progressBar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_rubert-tiny2\\\\tokenizer_config.json',\n",
       " 'tokenizer_rubert-tiny2\\\\special_tokens_map.json',\n",
       " 'tokenizer_rubert-tiny2\\\\vocab.txt',\n",
       " 'tokenizer_rubert-tiny2\\\\added_tokens.json',\n",
       " 'tokenizer_rubert-tiny2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.eval()\n",
    "#model.save_pretrained(\"new_rubert-tiny2\")\n",
    "#tokenizer.save_pretrained(\"tokenizer_rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "    (position_embeddings): Embedding(2048, 312)\n",
       "    (token_type_embeddings): Embedding(2, 312)\n",
       "    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-2): 3 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"new_rubert-tiny2\").to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2240937425541198327ffc34b835b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progressBar = tqdm(range(len(train_data)))\n",
    "queries_emb = []\n",
    "passages_emb = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(train_data)):\n",
    "        query = tokenizer(train_data[i][\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "        queries_emb.append(query_emb.cpu())\n",
    "\n",
    "        passage = tokenizer(train_data[i][\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "        passages_emb.append(passage_emb.cpu())\n",
    "\n",
    "        progressBar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_emb_tuples = [tuple(x[0].numpy().tolist()) for x in passages_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712ecfadbc8e44d397dc1e0ac573c43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuple_to_indices = defaultdict(list)\n",
    "for idx, tup in enumerate(passages_emb_tuples):\n",
    "    tuple_to_indices[tup].append(idx)\n",
    "\n",
    "progressBar = tqdm(range(len(train_data)))\n",
    "number_range = set(range(len(train_data)))\n",
    "pool_size = 5000\n",
    "filtered = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    query_emb = queries_emb[i]\n",
    "    \n",
    "    forbidden_indices = set(tuple_to_indices[passages_emb_tuples[i]])\n",
    "    forbidden_indices.add(i)\n",
    "\n",
    "    available_indices = list(number_range - forbidden_indices)\n",
    "\n",
    "    pool = random.sample(available_indices, pool_size)\n",
    "\n",
    "    top_list = []\n",
    "    for j in pool:\n",
    "        passage_emb = passages_emb[j]\n",
    "        cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "        top_list.append(cos_sim)\n",
    "\n",
    "    top_list.sort(reverse=True)\n",
    "\n",
    "    passage_emb = passages_emb[i]\n",
    "    cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "\n",
    "    if cos_sim < top_list[1]:\n",
    "        filtered.append(i)\n",
    "\n",
    "    progressBar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8699\n",
      "45328\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered_array.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
