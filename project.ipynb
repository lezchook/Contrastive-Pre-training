{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from utils import get_data, get_batches, validate_batches\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightning import LightningModule\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "# \"petkopetkov/medical-question-answering-all\"\n",
    "#\"tom-010/google_natural_questions_answerability\"\n",
    "DATASET_NAME = \"petkopetkov/medical-question-answering-all\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_ON_FILTERED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(dataset[\"train\"]['input'], dataset[\"train\"][\"output\"], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [{\"question\": q, \"context\": c} for q, c in zip(dataset[\"train\"]['input'], dataset[\"train\"][\"output\"]) if c is not None]\n",
    "valid_data = [{\"question\": q, \"context\": c} for q, c in zip(dataset[\"validation\"]['input'], dataset[\"validation\"][\"output\"]) if c is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_ON_FILTERED:\n",
    "    with open(\"filtered_array.txt\", \"r\") as f:\n",
    "        filtered = list(map(int, f.read().split()))\n",
    "    \n",
    "    indices = set(range(len(train_data))) - set(filtered)\n",
    "    queries_train, passages_train = get_data(indices, train_data)\n",
    "\n",
    "    with open(\"filtered_array_val.txt\", \"r\") as f:\n",
    "        filtered = list(map(int, f.read().split()))\n",
    "    \n",
    "    indices = set(range(len(valid_data))) - set(filtered)\n",
    "    queries_valid, passages_valid = get_data(indices, valid_data)\n",
    "\n",
    "else:\n",
    "    queries_train, passages_train = get_data(range(len(train_data)), train_data)\n",
    "    queries_valid, passages_valid = get_data(range(len(valid_data)), valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "\n",
    "    def forward(self, query, passage, negative_passages, temperature):\n",
    "        s_positive = F.cosine_similarity(query, passage, dim=-1) / temperature\n",
    "        s_negative = F.cosine_similarity(query.unsqueeze(1), negative_passages, dim=-1) / temperature\n",
    "\n",
    "        exp_for_sum = torch.cat([s_positive.unsqueeze(-1), s_negative], dim=-1)\n",
    "        log_exp_sum = torch.logsumexp(exp_for_sum, dim=-1)\n",
    "        \n",
    "        return (-s_positive + log_exp_sum).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "hf_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "train_data_batched = get_batches(queries_train, passages_train, BATCH_SIZE)\n",
    "valid_data_batched = get_batches(queries_valid, passages_valid, BATCH_SIZE)\n",
    "\n",
    "trainloader = DataLoader(train_data_batched, batch_size=None, collate_fn=lambda x: x, shuffle=True)\n",
    "validloader = DataLoader(valid_data_batched, batch_size=None, collate_fn=lambda x: x, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_batches(train_data_batched)\n",
    "validate_batches(valid_data_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitContrastiveModel(LightningModule):\n",
    "    def __init__(self, model, tokenizer, loss_fn, lr, weight_decay, warmup_ratio, epochs, train_len):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.epochs = epochs\n",
    "        self.train_len = train_len\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        query = tokenizer(batch[\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        passage = tokenizer(batch[\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        query_emb = self.model(**query).last_hidden_state.mean(dim=1)\n",
    "        passage_emb = self.model(**passage).last_hidden_state.mean(dim=1)\n",
    "        return query_emb, passage_emb\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        query_emb, passage_emb = self(batch)\n",
    "        \n",
    "        negative_passages = []\n",
    "        for i in range(len(passage_emb)):\n",
    "            negatives = torch.cat([passage_emb[:i], passage_emb[i + 1:]])\n",
    "            negative_passages.append(negatives)\n",
    "        negative_passages = torch.stack(negative_passages)\n",
    "\n",
    "        loss = self.loss_fn(query_emb, passage_emb, negative_passages, 0.01)\n",
    "        self.log(\"train_loss\", loss, batch_size=len(batch))\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        query_emb, passage_emb = self(batch)\n",
    "        \n",
    "        negative_passages = []\n",
    "        for i in range(len(passage_emb)):\n",
    "            negatives = torch.cat([passage_emb[:i], passage_emb[i + 1:]])\n",
    "            negative_passages.append(negatives)\n",
    "        negative_passages = torch.stack(negative_passages)\n",
    "\n",
    "        loss = self.loss_fn(query_emb, passage_emb, negative_passages, 0.01)\n",
    "        self.log(\"val_loss\", loss, batch_size=len(batch), on_epoch=True, prog_bar=False, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = self.trainer.callback_metrics.get(\"val_loss\")\n",
    "        if val_loss is not None:\n",
    "            print(f\"Validation Loss: {self.current_epoch + 1}: {val_loss:.4f}\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        total_steps = self.train_len * self.epochs\n",
    "        warmup_steps = int(total_steps * self.warmup_ratio)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitContrastiveModel(\n",
    "    model=hf_model,\n",
    "    tokenizer=tokenizer,\n",
    "    loss_fn=ContrastiveLoss(),\n",
    "    lr=9e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    train_len=len(trainloader)\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    filename=\"saved_model\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=trainloader, val_dataloaders=validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(\"2_new_rubert-tiny2\")\n",
    "tokenizer.save_pretrained(\"2_tokenizer_rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./1_tokenizer_rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"./1_new_rubert-tiny2\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "progressBar = tqdm(range(len(valid_data)))\n",
    "queries_emb = []\n",
    "passages_emb = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(valid_data)):\n",
    "        query = tokenizer(valid_data[i][\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "        queries_emb.append(query_emb.cpu())\n",
    "\n",
    "        passage = tokenizer(valid_data[i][\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "        passages_emb.append(passage_emb.cpu())\n",
    "\n",
    "        progressBar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_emb_tuples = [tuple(x[0].numpy().tolist()) for x in passages_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_to_indices = defaultdict(list)\n",
    "for idx, tup in enumerate(passages_emb_tuples):\n",
    "    tuple_to_indices[tup].append(idx)\n",
    "\n",
    "progressBar = tqdm(range(len(valid_data)))\n",
    "number_range = set(range(len(valid_data)))\n",
    "pool_size = 500\n",
    "filtered = []\n",
    "\n",
    "for i in range(len(valid_data)):\n",
    "    query_emb = queries_emb[i]\n",
    "    \n",
    "    forbidden_indices = set(tuple_to_indices[passages_emb_tuples[i]])\n",
    "    forbidden_indices.add(i)\n",
    "\n",
    "    available_indices = list(number_range - forbidden_indices)\n",
    "\n",
    "    pool = random.sample(available_indices, pool_size)\n",
    "\n",
    "    top_list = []\n",
    "    for j in pool:\n",
    "        passage_emb = passages_emb[j]\n",
    "        cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "        top_list.append(cos_sim)\n",
    "\n",
    "    top_list.sort(reverse=True)\n",
    "\n",
    "    passage_emb = passages_emb[i]\n",
    "    cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "\n",
    "    if cos_sim < top_list[10]:\n",
    "        filtered.append(i)\n",
    "\n",
    "    progressBar.update(1)\n",
    "\n",
    "with open(\"filtered_array_val.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, filtered)))\n",
    "    \n",
    "print(len(filtered))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
