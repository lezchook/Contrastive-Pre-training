{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from utils import get_data, get_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/rubert-tiny2\"\n",
    "DATASET_NAME = \"sberquad\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_ON_FILTERED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"validation\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_valid, passages_valid = get_data(range(len(valid_data)), valid_data)\n",
    "\n",
    "if TRAIN_ON_FILTERED:\n",
    "    with open(\"filtered_array.txt\", \"r\") as f:\n",
    "        filtered = list(map(int, f.read().split()))\n",
    "    \n",
    "    indices = set(range(len(train_data))) - set(filtered)\n",
    "    queries_train, passages_train = get_data(indices, train_data)\n",
    "else:\n",
    "    queries_train, passages_train = get_data(range(len(train_data)), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "\n",
    "    def forward(self, query, passage, negative_passages, temperature):\n",
    "        s_positive = F.cosine_similarity(query, passage, dim=-1) / temperature\n",
    "        s_negative = F.cosine_similarity(query.unsqueeze(1), negative_passages, dim=-1) / temperature\n",
    "\n",
    "        exp_for_sum = torch.cat([s_positive.unsqueeze(-1), s_negative], dim=-1)\n",
    "        log_exp_sum = torch.logsumexp(exp_for_sum, dim=-1)\n",
    "        \n",
    "        return (-s_positive + log_exp_sum).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "train_data_batched = get_batches(queries_train, passages_train, BATCH_SIZE)\n",
    "valid_data_batched = get_batches(queries_valid, passages_valid, BATCH_SIZE)\n",
    "\n",
    "trainloader = DataLoader(train_data_batched, batch_size=None, collate_fn=lambda x: x, shuffle=True)\n",
    "validloader = DataLoader(valid_data_batched, batch_size=None, collate_fn=lambda x: x, shuffle=True)\n",
    "\n",
    "loss_function = ContrastiveLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(trainloader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    progressBar = tqdm(range(len(trainloader)), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in trainloader:\n",
    "        query = tokenizer(batch[\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        passage = tokenizer(batch[\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "\n",
    "        query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "        passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        negative_passages = []\n",
    "        for i in range(len(passage_emb)):\n",
    "            negatives = torch.cat([passage_emb[:i], passage_emb[i + 1:]])\n",
    "            negative_passages.append(negatives)\n",
    "\n",
    "        negative_passages = torch.stack(negative_passages)\n",
    "\n",
    "        loss = loss_function(query_emb, passage_emb, negative_passages, 0.01)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progressBar.update(1)\n",
    "        total_loss += loss.item()\n",
    "        progressBar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validloader:\n",
    "            query = tokenizer(batch[\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "            passage = tokenizer(batch[\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "\n",
    "            query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "            passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            negative_passages = []\n",
    "            for i in range(len(passage_emb)):\n",
    "                negatives = torch.cat([passage_emb[:i], passage_emb[i + 1:]])\n",
    "                negative_passages.append(negatives)\n",
    "\n",
    "            negative_passages = torch.stack(negative_passages)\n",
    "\n",
    "            loss = loss_function(query_emb, passage_emb, negative_passages, 0.01)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(trainloader)}, Valid_Loss: {valid_loss / len(validloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.save_pretrained(\"new_rubert-tiny2\")\n",
    "tokenizer.save_pretrained(\"tokenizer_rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"1_tokenizer_rubert-tiny2\")\n",
    "# #model = AutoModel.from_pretrained(\"1_new_rubert-tiny2\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "progressBar = tqdm(range(len(train_data)))\n",
    "queries_emb = []\n",
    "passages_emb = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(train_data)):\n",
    "        query = tokenizer(train_data[i][\"question\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        query_emb = model(**query).last_hidden_state.mean(dim=1)\n",
    "        queries_emb.append(query_emb.cpu())\n",
    "\n",
    "        passage = tokenizer(train_data[i][\"context\"], return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        passage_emb = model(**passage).last_hidden_state.mean(dim=1)\n",
    "        passages_emb.append(passage_emb.cpu())\n",
    "\n",
    "        progressBar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_emb_tuples = [tuple(x[0].numpy().tolist()) for x in passages_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_to_indices = defaultdict(list)\n",
    "for idx, tup in enumerate(passages_emb_tuples):\n",
    "    tuple_to_indices[tup].append(idx)\n",
    "\n",
    "progressBar = tqdm(range(len(train_data)))\n",
    "number_range = set(range(len(train_data)))\n",
    "pool_size = 5000\n",
    "filtered = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    query_emb = queries_emb[i]\n",
    "    \n",
    "    forbidden_indices = set(tuple_to_indices[passages_emb_tuples[i]])\n",
    "    forbidden_indices.add(i)\n",
    "\n",
    "    available_indices = list(number_range - forbidden_indices)\n",
    "\n",
    "    pool = random.sample(available_indices, pool_size)\n",
    "\n",
    "    top_list = []\n",
    "    for j in pool:\n",
    "        passage_emb = passages_emb[j]\n",
    "        cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "        top_list.append(cos_sim)\n",
    "\n",
    "    top_list.sort(reverse=True)\n",
    "\n",
    "    passage_emb = passages_emb[i]\n",
    "    cos_sim = F.cosine_similarity(query_emb, passage_emb, dim=-1).item()\n",
    "\n",
    "    if cos_sim < top_list[1]:\n",
    "        filtered.append(i)\n",
    "\n",
    "    progressBar.update(1)\n",
    "\n",
    "with open(\"filtered_array.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, filtered)))\n",
    "    \n",
    "print(len(filtered))\n",
    "print(len(train_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
